{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffersondemota/projeto_/blob/main/comparador(v_14).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "c9e13a55",
        "outputId": "0ab0e9de-1a04-4950-981d-50dfd84e22ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ee1a0fc5-2c25-4f2e-b6ba-d1e652e187fa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ee1a0fc5-2c25-4f2e-b6ba-d1e652e187fa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving logo-branca-qfpsii0s3y2kjypwqu57rvsp16k4hj6dc0rz3dj1ia.png to logo-branca-qfpsii0s3y2kjypwqu57rvsp16k4hj6dc0rz3dj1ia (2).png\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbpW1SRItav-",
        "outputId": "f6b88ad2-cac0-4e10-fad3-64145ca4b000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Arquivos criados: app.py e run_in_colab.py\n",
            "/content/run_in_colab.py:9: DeprecationWarning: 'pkgutil.find_loader' is deprecated and slated for removal in Python 3.14; use importlib.util.find_spec() instead\n",
            "  need=[p for p in pkgs if pkgutil.find_loader(p) is None]\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "üåê Public URL: https://sammy-cybernetic-denice.ngrok-free.dev\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://136.114.250.233:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-12-31 18:27:43.001 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:28:57.835 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:29:05.669 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:29:13.245 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:29:25.227 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:29:27.393 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:29:51.519 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:29:53.793 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "/content/app.py:604: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  by_day = audit_5d.groupby(\"DATA\", dropna=False).apply(_agg_day).reset_index()\n",
            "2025-12-31 18:31:11.102 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'` or specify an integer width.\n",
            "2025-12-31 18:31:11.109 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'` or specify an integer width.\n",
            "2025-12-31 18:31:11.116 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'` or specify an integer width.\n",
            "2025-12-31 18:36:48.105 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:36:48.844 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:36:56.706 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:37:05.876 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:37:06.563 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:37:23.035 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:37:23.703 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:37:24.226 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:37:33.283 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-31 18:37:36.100 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "/content/app.py:604: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  by_day = audit_5d.groupby(\"DATA\", dropna=False).apply(_agg_day).reset_index()\n",
            "2025-12-31 18:37:49.357 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'` or specify an integer width.\n",
            "2025-12-31 18:37:49.366 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'` or specify an integer width.\n",
            "2025-12-31 18:37:49.376 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'` or specify an integer width.\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# üß≠ Sistema de Compara√ß√£o de Custos Log√≠sticos\n",
        "# Frota Pr√≥pria x Agregada x CrossDocking\n",
        "# Execut√°vel direto no Google Colab via Streamlit + ngrok\n",
        "# =========================================================\n",
        "\n",
        "# === 1Ô∏è‚É£ Cria√ß√£o dos arquivos app.py e run_in_colab.py ===\n",
        "\n",
        "app_code = r'''\n",
        "import os\n",
        "import io\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from datetime import datetime, timedelta\n",
        "from PIL import Image # Import Pillow library to handle images\n",
        "import altair as alt # Import altair with alias alt\n",
        "import sys, subprocess\n",
        "from math import sqrt\n",
        "import re # Import regex module\n",
        "\n",
        "# --- Fun√ß√µes auxiliares (resumidas para clareza) ---\n",
        "def safe_float(x, default=0.0):\n",
        "    try: return float(str(x).replace(\",\", \".\"))\n",
        "    except: return default\n",
        "\n",
        "def to_hours(x):\n",
        "    if pd.isna(x): return 0\n",
        "    s=str(x)\n",
        "    if \":\" in s:\n",
        "        h,m,*_ = s.split(\":\")\n",
        "        return float(h)+float(m)/60\n",
        "    try:\n",
        "        # Assume numerical input is in minutes and convert to hours\n",
        "        return float(s) / 60\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def _money_to_float(x):\n",
        "    \"\"\"\n",
        "    Converte strings de moeda para float, aceitando formatos:\n",
        "    'R$ 3.120,00' | '3.120,00' | '3120.00' | 3120\n",
        "    \"\"\"\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    s = str(x).strip()\n",
        "    # remove R$, espa√ßos e quaisquer s√≠mbolos n√£o num√©ricos (mant√©m '.' e ',')\n",
        "    s = re.sub(r'[^\\d,.\\-]', '', s)\n",
        "    # se possui milhares com ponto e decimais com v√≠rgula ‚Üí troca padr√£o BR pra padr√£o US\n",
        "    if s.count(',') == 1 and s.rfind(',') > s.rfind('.'):\n",
        "        s = s.replace('.', '').replace(',', '.')\n",
        "    else:\n",
        "        s = s.replace(',', '.')\n",
        "    try:\n",
        "        return float(s)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def _normalize_manual_table(df_manual: pd.DataFrame) -> pd.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Normaliza a tabela manual:\n",
        "      - Colunas para UPPER\n",
        "      - DATA_ROTA -> DATA (date)\n",
        "      - Converte CUSTO_FROTA_AGREGADA para float\n",
        "      - Mant√©m apenas colunas necess√°rias\n",
        "      - Remove duplicados por (DATA, IDVEICULO), mantendo a √∫ltima ocorr√™ncia\n",
        "    \"\"\"\n",
        "    if df_manual is None:\n",
        "        return None\n",
        "\n",
        "    # padroniza nomes\n",
        "    dfm = df_manual.copy()\n",
        "    dfm.columns = [c.strip().upper() for c in dfm.columns]\n",
        "\n",
        "    # tenta mapear a coluna de data\n",
        "    if \"DATA\" in dfm.columns:\n",
        "        pass\n",
        "    elif \"DATA_ROTA\" in dfm.columns:\n",
        "        dfm.rename(columns={\"DATA_ROTA\": \"DATA\"}, inplace=True)\n",
        "    else:\n",
        "        st.warning(\"A Tabela Manual n√£o possui coluna 'DATA' ou 'DATA_ROTA'. Ignorando substitui√ß√£o do custo agregado.\")\n",
        "        return None\n",
        "\n",
        "    required = {\"DATA\", \"IDVEICULO\", \"CUSTO_FROTA_AGREGADA\"}\n",
        "    if not required.issubset(dfm.columns):\n",
        "        st.warning(f\"Tabela Manual n√£o cont√©m as colunas esperadas: {sorted(list(required))}. Ignorando substitui√ß√£o do custo agregado.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    # normaliza DATA para date\n",
        "    dfm[\"DATA\"] = pd.to_datetime(dfm[\"DATA\"], errors=\"coerce\").dt.date\n",
        "\n",
        "    # normaliza custo para float\n",
        "    dfm[\"CUSTO_FROTA_AGREGADA\"] = dfm[\"CUSTO_FROTA_AGREGADA\"].apply(_money_to_float)\n",
        "\n",
        "    # mant√©m apenas colunas necess√°rias e remove linhas com DATA/IDVEICULO inv√°lidos ou custo NaN\n",
        "    dfm = dfm[[\"DATA\", \"IDVEICULO\", \"CUSTO_FROTA_AGREGADA\"]].dropna(subset=[\"DATA\", \"IDVEICULO\", \"CUSTO_FROTA_AGREGADA\"])\n",
        "\n",
        "    # remove duplicidades por DATA+IDVEICULO (mant√©m a √∫ltima)\n",
        "    dfm = dfm.sort_values([\"DATA\", \"IDVEICULO\"]).drop_duplicates(subset=[\"DATA\", \"IDVEICULO\"], keep=\"last\")\n",
        "\n",
        "    return dfm\n",
        "\n",
        "\n",
        "# Modified OSRM function to return polyline\n",
        "def osrm_distance_duration_polyline(coords):\n",
        "    if len(coords)<2: return 0,0,None\n",
        "    base = \"https://router.project-osrm.org/route/v1/driving/\"\n",
        "    path = \";\".join([f\"{c[0]:.6f},{c[1]:.6f}\" for c in coords])\n",
        "    url = f\"{base}{path}?overview=full&geometries=polyline6\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=20)\n",
        "        if r.status_code == 200:\n",
        "            data = r.json()\n",
        "            if data.get(\"routes\"):\n",
        "                route = data[\"routes\"][0]\n",
        "                dist_km = route[\"distance\"] / 1000.0\n",
        "                dur_h = route[\"duration\"] / 3600.0\n",
        "                poly6 = route.get(\"geometry\")  # polyline for the toll API\n",
        "                return dist_km, dur_h, poly6\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0, 0.0, None\n",
        "\n",
        "\n",
        "def vehicle_class(p):\n",
        "    if p<=600: return \"UTILITARIO_600\"\n",
        "    elif p<=1600: return \"VAN_1600\"\n",
        "    elif p<=3000: return \"TRES_QUARTOS_3000\"\n",
        "    elif p<=5000: return \"SEMI_TOCO_5000\"\n",
        "    elif p<=14000: return \"TRUCK\"\n",
        "    else: return \"CARRETA\"\n",
        "\n",
        "def map_col(v):\n",
        "    return {\n",
        "        \"UTILITARIO_600\":\"FRETE UTILIT√ÅRIO 600Kg\",\n",
        "        \"VAN_1600\":\"FRETE VAN 1600Kg\",\n",
        "        \"TRES_QUARTOS_3000\":\"FRETE 3/4 3000Kg\",\n",
        "        \"SEMI_TOCO_5000\":\"FRETE SEMI - TOCO 5000Kg\",\n",
        "        \"TRUCK\":\"FRETE TRUCK\",\n",
        "        \"CARRETA\":\"FRETE CARRETA\"\n",
        "    }.get(v)\n",
        "\n",
        "def interpolate(df, km_col, val_col, km):\n",
        "    if val_col not in df.columns: return np.nan\n",
        "    df=df.sort_values(km_col)\n",
        "    x=df[km_col].astype(float).values; y=df[val_col].astype(float).values\n",
        "    if km<=x.min(): return y[0]\n",
        "    if km>=x.max(): return y[-1]\n",
        "    i=np.searchsorted(x,km)\n",
        "    x0,x1=x[i-1],x[i]; y0,y1=y[i-1],y[i]\n",
        "    return y0+(y1-y0)*(km-x0)/(x1-x0)\n",
        "\n",
        "# New function to map vehicle class to axles\n",
        "def eixo_por_classe(vclass):\n",
        "    # adjust according to your real fleet\n",
        "    return {\n",
        "        \"UTILITARIO_600\": 2,\n",
        "        \"VAN_1600\": 2,\n",
        "        \"TRES_QUARTOS_3000\": 2,\n",
        "        \"SEMI_TOCO_5000\": 3,\n",
        "        \"TRUCK\": 3,\n",
        "        \"CARRETA\": 5,   # can be 5‚Äì9; adjust according to trailer\n",
        "    }.get(vclass, 2)\n",
        "\n",
        "# New function to call TollGuru API (provided by user)\n",
        "def toll_cost_for_route_tollguru(polyline6, vehicle_axles, api_key=None):\n",
        "    \"\"\"\n",
        "    Calcula ped√°gio via TollGuru a partir de uma polilinha (polyline6) e eixos do ve√≠culo.\n",
        "    Retorna custo total em BRL ou 0.0 se falhar.\n",
        "    \"\"\"\n",
        "    if not api_key or not polyline6:\n",
        "        return 0.0\n",
        "    url = \"https://dev.tollguru.com/v1/calc/route\"\n",
        "    headers = {\"x-api-key\": api_key, \"Content-Type\": \"application/json\"}\n",
        "    payload = {\n",
        "        \"source\": \"OSRM\",\n",
        "        \"polyline\": polyline6,\n",
        "        \"vehicleType\": \"truck\",        # ou \"car\" se aplic√°vel\n",
        "        \"vehicle\": {\n",
        "            \"axles\": vehicle_axles\n",
        "        },\n",
        "        \"country\": \"BR\", # Changed to fixed \"BR\"\n",
        "        \"currency\": \"BRL\"\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.post(url, headers=headers, json=payload, timeout=30)\n",
        "        if resp.status_code == 200:\n",
        "            data = resp.json()\n",
        "            # verifique o caminho do total (varia por vers√£o); exemplo comum:\n",
        "            total = (\n",
        "                data.get(\"route\", {})\n",
        "                    .get(\"costs\", {})\n",
        "                    .get(\"tag\", {})\n",
        "                    .get(\"currency\", \"BRL\")\n",
        "            )\n",
        "            # fallback direto:\n",
        "            total_val = data.get(\"route\", {}).get(\"costs\", {}).get(\"tag\", {}).get(\"amount\")\n",
        "            if total_val is None:\n",
        "                # outro formato poss√≠vel\n",
        "                total_val = data.get(\"route\", {}).get(\"costs\", {}).get(\"cash\", {}).get(\"amount\", 0.0)\n",
        "            return float(total_val or 0.0)\n",
        "        else:\n",
        "             st.warning(f\"TollGuru API returned status code: {resp.status_code} - {resp.text}\")\n",
        "             return 0.0\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Error calling TollGuru API: {e}\")\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Comparador de Custos\", layout=\"wide\")\n",
        "# Add the image logo\n",
        "logo_path = \"/content/logo-branca-qfpsii0s3y2kjypwqu57rvsp16k4hj6dc0rz3dj1ia.png\"\n",
        "\n",
        "if os.path.exists(logo_path):\n",
        "    st.image(logo_path, use_container_width=False)\n",
        "else:\n",
        "    st.warning(\"‚ö†Ô∏è Logo image not found. Fa√ßa upload do arquivo 'logo-branca-qfpsii0s3y2kjypwqu57rvsp16k4hj6dc0rz3dj1ia.png' no diret√≥rio /content/.\")\n",
        "\n",
        "st.title(\"üöõ An√°lise Comparativa das Modalidades Operacionais de Transporte: Frota Pr√≥pria, Agregado e Cross-Docking\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Par√¢metros\")\n",
        "    total_fixo_per_day = 147.36 # Modified variable name with new value\n",
        "    st.write(\"**Custo fixo total/dia:**\",round(total_fixo_per_day,2)) # Used new variable name\n",
        "    km_l=st.number_input(\"Consumo (km/L)\",5.0)\n",
        "    preco_comb=st.number_input(\"Pre√ßo Combust√≠vel\",5.99)\n",
        "\n",
        "    # Added selectbox for toll method\n",
        "    pedagio_provider = st.selectbox(\"Ped√°gio - M√©todo\", [\"manual (taxa R√°/km)\", \"TollGuru API\"])\n",
        "    per_km_toll = st.number_input(\"Taxa Ped√°gio (R$/km)\", min_value=0.0, value=0.45, step=0.05, format=\"%.2f\")\n",
        "    tollguru_key = st.text_input(\"TollGuru API Key\", type=\"password\")\n",
        "\n",
        "\n",
        "    j_ini,j_fim=6,16\n",
        "    # Removed custo_h_extra input as it will be calculated\n",
        "    base_lat=st.number_input(\"Base Lat\",-23.603)\n",
        "    base_lon=st.number_input(\"Base Lon\",-46.919)\n",
        "\n",
        "    st.subheader(\"Uploads\")\n",
        "    cli=st.file_uploader(\"RelatorioCadastroCliente\")\n",
        "    rota=st.file_uploader(\"rota\")\n",
        "    tab=st.file_uploader(\"TabelaFreteCustosFrotaAgregada\")\n",
        "    cross=st.file_uploader(\"CustosCrossDocking\")\n",
        "    # Added new file uploader for manual table\n",
        "    tabela_manual = st.file_uploader(\"Tabela Manual (Substituir Custo Agregado)\")\n",
        "    run=st.button(\"üöÄ Executar\")\n",
        "\n",
        "def read_df(f):\n",
        "    if f is None: return None\n",
        "    n=f.name.lower()\n",
        "    if n.endswith(\".csv\"): return pd.read_csv(f)\n",
        "    return pd.read_excel(f)\n",
        "\n",
        "if run:\n",
        "    df_cli,df_rota,df_tab,df_cross,df_tabela_manual=[read_df(x) for x in [cli,rota,tab,cross,tabela_manual]] # Added df_tabela_manual\n",
        "    if any(df is None for df in [df_cli,df_rota,df_tab,df_cross]):\n",
        "        st.error(\"Faltam planilhas obrigat√≥rias!\"); st.stop() # Modified error message\n",
        "    if \"TollGuru\" in pedagio_provider and not tollguru_key: # Check if API key is provided when TollGuru is selected\n",
        "        st.error(\"TollGuru API Key √© necess√°ria para calcular ped√°gios.\"); st.stop()\n",
        "\n",
        "    df_cli.rename(columns=lambda x:x.strip(),inplace=True)\n",
        "    df_rota.rename(columns=lambda x:x.strip().upper(),inplace=True)\n",
        "    df_tab.rename(columns=lambda x:x.strip(),inplace=True)\n",
        "    df_cross.rename(columns=lambda x:x.strip().upper(),inplace=True)\n",
        "    df_cli_m=df_cli[[\"C√≥digoCliente\",\"Latitude\",\"Longitude\",\"TempoM√©dioEntrega\", \"Segmento\"]] # Included Segmento column\n",
        "    df_cli_m.columns=[\"IDCLI\",\"LAT\",\"LON\",\"TEMPO\", \"SEGMENTO\"] # Renamed Segmento column\n",
        "    df=df_rota.merge(df_cli_m,on=\"IDCLI\",how=\"left\")\n",
        "\n",
        "    resultados=[]\n",
        "    detalhe_frota_propria = [] # New list for detailed own fleet cost\n",
        "    detalhe_tempo_jornada = [] # New list for detailed journey time\n",
        "    base=(base_lon,base_lat)\n",
        "    custo_motorista_fixo = 164.25 # Fixed driver cost\n",
        "    custo_indireto_fixo = 772.10 # Fixed indirect cost\n",
        "    custo_ajudante_fixo = 127.53 # Fixed helper cost\n",
        "\n",
        "    # Calculate number of unique vehicles before the loop\n",
        "    num_unique_vehicles = df_rota['IDVEICULO'].nunique()\n",
        "    # Calculate the divided indirect cost\n",
        "    custo_indireto_dividido = custo_indireto_fixo / num_unique_vehicles if num_unique_vehicles > 0 else 0\n",
        "\n",
        "    # --- localizar coluna de data na planilha rota ---\n",
        "    def _detect_date_column(df_rota: pd.DataFrame):\n",
        "        candidates = [\"DATA\", \"DATA_ROTA\", \"DATA_ENTREGA\", \"DT_CARGA\", \"DATA_SAIDA\"]\n",
        "        for c in candidates:\n",
        "            if c in df_rota.columns:\n",
        "                return c\n",
        "        # fallback: tenta achar por dtype datetime\n",
        "        for c in df_rota.columns:\n",
        "            if np.issubdtype(df_rota[c].dtype, np.datetime64):\n",
        "                return c\n",
        "        return None\n",
        "\n",
        "    date_col = _detect_date_column(df_rota)\n",
        "    if date_col is None:\n",
        "        st.warning(\"‚ö†Ô∏è N√£o encontrei coluna de data em 'rota'. Adicionando data padr√£o (hoje) para auditoria.\")\n",
        "        df_rota[\"_DATA_AUD\"] = pd.Timestamp(\"today\").normalize()\n",
        "        date_col = \"_DATA_AUD\"\n",
        "    else:\n",
        "        # normaliza para date (sem hora)\n",
        "        df_rota[date_col] = pd.to_datetime(df_rota[date_col], errors=\"coerce\").dt.date\n",
        "\n",
        "\n",
        "    # Change grouping to process by both date and vehicle ID\n",
        "    for (current_date, vid), g in df.groupby([date_col, \"IDVEICULO\"]):\n",
        "        coords=[base]+[(float(x.LON),float(x.LAT)) for _,x in g.iterrows()]+[base]\n",
        "        km,hr, polyline = osrm_distance_duration_polyline(coords) # Updated to receive polyline\n",
        "        tempo=g[\"TEMPO\"].apply(to_hours).sum()\n",
        "        total_h=hr+tempo\n",
        "        extra=max(0,total_h-(j_fim-j_ini))\n",
        "        # Calculate Hora trabalhada and Custo Hora Extra based on MDO\n",
        "        custo_motorista = custo_motorista_fixo\n",
        "        # Use the divided indirect cost\n",
        "        custo_indireto = custo_indireto_dividido\n",
        "        custo_ajudante = custo_ajudante_fixo # Added helper cost\n",
        "\n",
        "        # Check if any client in the route has \"Segmento\" as \"Atacado\"\n",
        "        if \"Atacado\" in g[\"SEGMENTO\"].values:\n",
        "            custo_ajudante = 0 # Set custo_ajudante to 0 if any client is Atacado\n",
        "\n",
        "\n",
        "        # Check if number of unique clients is greater than 1 and double helper cost (only if not Atacado)\n",
        "        num_unique_clients = g[\"IDCLI\"].nunique()\n",
        "        if num_unique_clients > 1 and \"Atacado\" not in g[\"SEGMENTO\"].values:\n",
        "            custo_ajudante = custo_ajudante * 2\n",
        "\n",
        "\n",
        "        total_custo_mdo = custo_motorista + custo_ajudante # Calculate total MDO cost\n",
        "        hora_trabalhada_rate = total_custo_mdo / 9 if total_custo_mdo > 0 else 0 # Calculate Hora trabalhada rate, avoid division by zero\n",
        "        custo_extra = extra * hora_trabalhada_rate * 1.40 # Calculate Custo Hora Extra with 40% premium\n",
        "\n",
        "        comb=(km/km_l)*preco_comb\n",
        "        # Calculate toll cost based on selected method\n",
        "        peso=g[\"PESO\"].sum()\n",
        "        vcl=vehicle_class(peso)\n",
        "        axles = eixo_por_classe(vcl) # Get axles based on vehicle class\n",
        "\n",
        "        if \"TollGuru\" in pedagio_provider:\n",
        "            ped = toll_cost_for_route_tollguru(polyline, axles, tollguru_key) # Calculate toll cost using API\n",
        "        else:\n",
        "            ped = km * per_km_toll # Calculate toll cost using manual rate\n",
        "\n",
        "        # aqui √© o calculo de ped√°gio\n",
        "        ped = ped * 2 # Multiply toll cost by 2 as requested\n",
        "        ped = ped + (axles * 1.0) # Add 1 real per axle\n",
        "\n",
        "        frota_propria=total_fixo_per_day+comb+ped+custo_extra+custo_motorista+custo_indireto+custo_ajudante # Included all fixed costs\n",
        "        col=map_col(vcl)\n",
        "        km_ida=km/2\n",
        "        frete=interpolate(df_tab,\"KM IDA\",col,km_ida) if col else np.nan\n",
        "\n",
        "        cross_v=peso * 0.38\n",
        "        # Append results using the date and vid from the group key\n",
        "        resultados.append([current_date, vid, round(km,1),peso,frota_propria,frete,cross_v,vcl, num_unique_clients])\n",
        "\n",
        "        # Append detailed own fleet cost for this vehicle, including all fixed costs and total MDO\n",
        "        detalhe_frota_propria.append([current_date, vid, total_fixo_per_day, comb, ped, custo_extra, custo_motorista, custo_indireto, custo_ajudante, total_custo_mdo, frota_propria])\n",
        "\n",
        "        # Append detailed journey time for this vehicle\n",
        "        detalhe_tempo_jornada.append([current_date, vid, tempo, hr, total_h, extra])\n",
        "\n",
        "\n",
        "    out=pd.DataFrame(resultados,columns=[\"DATA\", \"IDVEICULO\",\"KM_TOTAL\",\"PESO\",\"CUSTO_FROTA_PRORIA\",\"CUSTO_FROTA_AGREGADA\",\"CUSTO_CROSS\",\"VEICULO\", \"NUM_UNIQUE_CLIENTS\"]) # Added DATA\n",
        "\n",
        "\n",
        "    # =========================================================\n",
        "    # SUBSTITUIR CUSTO_FROTA_AGREGADA USANDO \"Tabela Manual (Substituir Custo Agregada)\"\n",
        "    # Requer colunas: DATA (ou DATA_ROTA), IDVEICULO, CUSTO_FROTA_AGREGADA\n",
        "    # =========================================================\n",
        "\n",
        "    if tabela_manual is not None:\n",
        "        try:\n",
        "            # Use read_df to handle both excel and csv\n",
        "            df_manual_raw = read_df(tabela_manual)\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Erro ao ler 'Tabela Manual (Substituir Custo Agregada)': {e}\")\n",
        "            df_manual_raw = None\n",
        "\n",
        "        df_manual_ok = _normalize_manual_table(df_manual_raw)\n",
        "\n",
        "        if df_manual_ok is not None and not df_manual_ok.empty:\n",
        "            st.info(f\"Aplicando {len(df_manual_ok)} substitui√ß√µes de CUSTO_FROTA_AGREGADA da Tabela Manual...\")\n",
        "            # garante que 'out' tem a coluna DATA como date e IDVEICULO como string para merge\n",
        "            out[\"DATA\"] = pd.to_datetime(out[\"DATA\"], errors=\"coerce\").dt.date\n",
        "            out[\"IDVEICULO\"] = out[\"IDVEICULO\"].astype(str)\n",
        "\n",
        "            # merge para trazer custo manual como coluna auxiliar\n",
        "            # df_manual_ok is indexed by (DATA, IDVEICULO)\n",
        "            # Reset index of df_manual_ok so DATA and IDVEICULO become columns for merging\n",
        "            df_manual_ok_reset = df_manual_ok.reset_index()\n",
        "\n",
        "            # Ensure 'DATA' and 'IDVEICULO' in the reset manual table are ready for merging\n",
        "            df_manual_ok_reset[\"DATA\"] = pd.to_datetime(df_manual_ok_reset[\"DATA\"], errors=\"coerce\").dt.date\n",
        "            df_manual_ok_reset['IDVEICULO'] = df_manual_ok_reset['IDVEICULO'].astype(str) # Ensure IDVEICULO is string\n",
        "\n",
        "\n",
        "            # merge out with the processed manual table on 'DATA' and 'IDVEICULO' columns\n",
        "            out = out.merge(\n",
        "                df_manual_ok_reset[['DATA', 'IDVEICULO', 'CUSTO_FROTA_AGREGADA']].rename(columns={'CUSTO_FROTA_AGREGADA': 'CUSTO_AGREGADA_MANUAL'}),\n",
        "                on=[\"DATA\", \"IDVEICULO\"], # Merge on columns now\n",
        "                how=\"left\"\n",
        "            )\n",
        "\n",
        "            # substitui CUSTO_FROTA_AGREGADA pelo manual quando dispon√≠vel\n",
        "            out[\"CUSTO_FROTA_AGREGADA\"] = np.where(\n",
        "                out[\"CUSTO_AGREGADA_MANUAL\"].notna(),\n",
        "                out[\"CUSTO_AGREGADA_MANUAL\"],\n",
        "                out[\"CUSTO_FROTA_AGREGADA\"]\n",
        "            )\n",
        "\n",
        "            # remove a auxiliar\n",
        "            out.drop(columns=[\"CUSTO_AGREGADA_MANUAL\"], inplace=True)\n",
        "\n",
        "            st.success(\"CUSTO_FROTA_AGREGADA substitu√≠do com base na Tabela Manual quando aplic√°vel.\")\n",
        "        elif df_manual_ok is not None and df_manual_ok.empty:\n",
        "             st.info(\"Tabela Manual carregada, mas vazia ou sem dados v√°lidos para substitui√ß√£o.\")\n",
        "        else:\n",
        "             st.warning(\"Tabela Manual n√£o processada corretamente. Substitui√ß√£o n√£o aplicada.\")\n",
        "    else:\n",
        "        st.info(\"Nenhuma Tabela Manual (Substituir Custo Agregado) carregada. Usando custos calculados.\")\n",
        "\n",
        "    # Recalculate MODAL_MAIS_BARATO after applying the manual override\n",
        "    out[\"MODAL_MAIS_BARATO\"]=out[[\"CUSTO_FROTA_PRORIA\",\"CUSTO_FROTA_AGREGADA\",\"CUSTO_CROSS\"]].idxmin(axis=1)\n",
        "\n",
        "    st.dataframe(out)\n",
        "\n",
        "\n",
        "    # Display the second table for detailed own fleet cost\n",
        "    st.subheader(\"Detalhe do Custo da Frota Propria\") # Subheader for the new table\n",
        "    df_detalhe = pd.DataFrame(detalhe_frota_propria, columns=[\"DATA\", \"IDVEICULO\", \"Custo Fixo Di√°rio\", \"Custo Combust√≠vel\", \"Custo Ped√°gio\", \"Custo Hora Extra\", \"Custo Motorista\", \"Custo ADM\", \"Custo Ajudante\", \"Total Custo MDO\", \"Total Frota Propria\"]) # New DataFrame, added DATA\n",
        "    st.dataframe(df_detalhe) # Display the new DataFrame\n",
        "\n",
        "    # Display the third table for detailed journey time\n",
        "    st.subheader(\"Detalhe do Tempo de Jornada\") # Subheader for the third table\n",
        "    df_tempo = pd.DataFrame(detalhe_tempo_jornada, columns=[\"DATA\", \"IDVEICULO\", \"Tempo M√©dio Entrega (horas)\", \"Tempo de Trajeto Estimado (horas)\", \"Total Horas Jornada\", \"Total de Hora Extra\"]) # New DataFrame, added DATA\n",
        "    st.dataframe(df_tempo) # Display the new DataFrame\n",
        "\n",
        "    # Create and display the new table for Operational Economy Analysis\n",
        "    st.subheader(\"An√°lise de Economia Operacional\")\n",
        "    # Ensure 'DATA' column in out is date objects for consistent merging\n",
        "    out['DATA'] = pd.to_datetime(out['DATA'], errors='coerce').dt.date\n",
        "    # Merge out with df_rota to get NOMETRANSPORTADORACRIACAO\n",
        "    # Use only necessary columns from df_rota for merging to avoid conflicts\n",
        "    # Use 'out' which now contains the potentially overridden CUSTO_FROTA_AGREGADA\n",
        "    df_economia = out.merge(df_rota[[\"IDVEICULO\", date_col, \"NOMETRANSPORTADORACRIACAO\"]].drop_duplicates(subset=[\"IDVEICULO\", date_col]), left_on=[\"IDVEICULO\", \"DATA\"], right_on=[\"IDVEICULO\", date_col], how=\"left\") # Merge on DATA and IDVEICULO\n",
        "\n",
        "    df_economia[\"MENOR_MODAL\"] = df_economia[[\"CUSTO_FROTA_PRORIA\", \"CUSTO_FROTA_AGREGADA\"]].idxmin(axis=1)\n",
        "    df_economia[\"MENOR_CUSTO\"] = df_economia[[\"CUSTO_FROTA_PRORIA\", \"CUSTO_FROTA_AGREGADA\"]].min(axis=1)\n",
        "\n",
        "    # Determine CUSTO_ESCOLHIDO based on NOMETRANSPORTADORACRIACAO\n",
        "    df_economia[\"CUSTO_ESCOLHIDO\"] = df_economia.apply(\n",
        "        lambda row: row[\"NOMETRANSPORTADORACRIACAO\"] if pd.isna(row[\"NOMETRANSPORTADORACRIACAO\"]) else (row[\"CUSTO_FROTA_PRORIA\"] if row[\"NOMETRANSPORTADORACRIACAO\"] == \"PROPRIO\" else row[\"CUSTO_FROTA_AGREGADA\"]),\n",
        "        axis=1\n",
        "    )\n",
        "    # convert CUSTO_ESCOLHIDO to numeric, coercing errors to NaN\n",
        "    df_economia[\"CUSTO_ESCOLHIDO\"] = pd.to_numeric(df_economia[\"CUSTO_ESCOLHIDO\"], errors='coerce')\n",
        "\n",
        "\n",
        "    df_economia[\"DIFERENCA_R$\"] = df_economia[\"CUSTO_ESCOLHIDO\"] - df_economia[\"MENOR_CUSTO\"]\n",
        "    df_economia[\"DIFERENCA_%\"] = np.where(df_economia[\"MENOR_CUSTO\"] > 0, df_economia[\"DIFERENCA_R$\"] / df_economia[\"MENOR_CUSTO\"] * 100.0, np.nan)\n",
        "\n",
        "\n",
        "    # Drop the NOMETRANSPORTADORACRIACAO and original date_col columns from the merged result if they exist and are not the final 'DATA' column\n",
        "    cols_to_drop = [\"NOMETRANSPORTADORACRIACAO\"]\n",
        "    # Check if date_col exists in df_economia before dropping\n",
        "    if date_col in df_economia.columns and date_col != \"DATA\":\n",
        "         cols_to_drop.append(date_col)\n",
        "    df_economia = df_economia.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "\n",
        "    # Reorder columns to have DATA and IDVEICULO at the beginning\n",
        "    economia_cols = [\"DATA\", \"IDVEICULO\"] + [col for col in df_economia.columns if col not in [\"DATA\", \"IDVEICULO\"]]\n",
        "    # Ensure all columns in economia_cols are actually in df_economia before reordering\n",
        "    economia_cols = [col for col in economia_cols if col in df_economia.columns]\n",
        "    df_economia = df_economia[economia_cols]\n",
        "\n",
        "\n",
        "    # Format currency and percentage columns\n",
        "    # Ensure columns exist and are numeric before formatting\n",
        "    for col in [\"CUSTO_FROTA_PRORIA\", \"CUSTO_FROTA_AGREGADA\", \"MENOR_CUSTO\", \"CUSTO_ESCOLHIDO\", \"DIFERENCA_R$\", \"DIFERENCA_%\"]:\n",
        "        if col in df_economia.columns:\n",
        "             # Convert to numeric first, coercing errors\n",
        "            df_economia[col] = pd.to_numeric(df_economia[col], errors='coerce')\n",
        "            if col == \"DIFERENCA_%\":\n",
        "                 df_economia[col] = df_economia[col].map('{:.2f}%'.format, na_action='ignore')\n",
        "            else:\n",
        "                 df_economia[col] = df_economia[col].map('{:.2f}'.format, na_action='ignore')\n",
        "\n",
        "\n",
        "\n",
        "    st.dataframe(df_economia)\n",
        "\n",
        "\n",
        "    # ============================\n",
        "    # AUDITORIA ‚Äì 5 DIAS (BLOCO)\n",
        "    # ============================\n",
        "\n",
        "    # --- garantir scipy/statsmodels para p-valores e ICs (instala se faltar) ---\n",
        "    def _ensure_stats_libs():\n",
        "        try:\n",
        "            import scipy, statsmodels  # noqa\n",
        "            return True\n",
        "        except Exception:\n",
        "            try:\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"scipy\", \"statsmodels\"])\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                st.warning(f\"N√£o foi poss√≠vel instalar scipy/statsmodels automaticamente: {e}\")\n",
        "                return False\n",
        "\n",
        "    _stats_ok = _ensure_stats_libs()\n",
        "    if _stats_ok:\n",
        "        import scipy.stats as stats\n",
        "    else:\n",
        "        stats = None\n",
        "\n",
        "    # --- construir base por IDVEICULO (1 linha por carga) com DATA e modal escolhido ---\n",
        "    # pega a 1¬™ data por ve√≠culo, e o modal escolhido informado\n",
        "    meta_rota = (\n",
        "        df_rota[[\"IDVEICULO\", date_col, \"NOMETRANSPORTADORACRIACAO\"]]\n",
        "        .dropna(subset=[\"IDVEICULO\"])\n",
        "        .drop_duplicates(subset=[\"IDVEICULO\", date_col]) # Include date_col in drop_duplicates\n",
        "        .rename(columns={date_col: \"DATA\"})\n",
        "    )\n",
        "\n",
        "    # junta com a tabela 'out' (que j√° tem os custos por IDVEICULO)\n",
        "    # Ensure 'DATA' column in meta_rota is date objects for consistent merging\n",
        "    meta_rota['DATA'] = pd.to_datetime(meta_rota['DATA'], errors='coerce').dt.date\n",
        "    audit = out.merge(meta_rota, on=[\"IDVEICULO\", \"DATA\"], how=\"left\") # Merge on DATA and IDVEICULO\n",
        "\n",
        "    # --- mapeia modal escolhido para custo escolhido (C_ESC) ---\n",
        "    def _map_modal_to_cost(row):\n",
        "        modal_raw = (row.get(\"NOMETRANSPORTADORACRIACAO\") or \"\").strip().upper()\n",
        "        if modal_raw in [\"PROPRIO\", \"PR√ìPRIO\", \"FROTA PROPRIA\", \"FROTA PR√ìPRIA\", \"PROPRIA\", \"PRIVADO\"]:\n",
        "            return row[\"CUSTO_FROTA_PRORIA\"]\n",
        "        if modal_raw in [\"AGREGADO\", \"TERCEIRO\", \"FROTA AGREGADA\"]:\n",
        "            return row[\"CUSTO_FROTA_AGREGADA\"]\n",
        "        if modal_raw in [\"CROSS\", \"CROSSDOCKING\", \"CD\", \"CROSS-DOCKING\"]:\n",
        "            return row[\"CUSTO_CROSS\"]\n",
        "        # fallback: se n√£o souber, assume custo agregado (para n√£o deixar NaN)\n",
        "        return row[\"CUSTO_FROTA_AGREGADA\"]\n",
        "\n",
        "    audit[\"C_ESC\"] = audit.apply(_map_modal_to_cost, axis=1)\n",
        "\n",
        "    # --- menor custo t√©cnico e modal √≥timo ---\n",
        "    audit[\"C_MIN\"] = audit[[\"CUSTO_FROTA_PRORIA\", \"CUSTO_FROTA_AGREGADA\", \"CUSTO_CROSS\"]].min(axis=1)\n",
        "    audit[\"MODAL_MIN\"] = audit[[\"CUSTO_FROTA_PRORIA\", \"CUSTO_FROTA_AGREGADA\", \"CUSTO_CROSS\"]].idxmin(axis=1)\n",
        "\n",
        "    # --- sucesso (acerto) e perdas ---\n",
        "    audit[\"SUCCESS\"] = (audit[\"C_ESC\"] == audit[\"C_MIN\"]).astype(int)\n",
        "    audit[\"DELTA_R$\"] = audit[\"C_ESC\"] - audit[\"C_MIN\"]\n",
        "    audit[\"DELTA_%\"] = np.where(audit[\"C_MIN\"] > 0, audit[\"DELTA_R$\"] / audit[\"C_MIN\"] * 100.0, np.nan)\n",
        "\n",
        "    # --- limitar para os 5 dias mais recentes com base em 'DATA' ---\n",
        "    if \"DATA\" in audit.columns and audit[\"DATA\"].notna().any():\n",
        "        # ordena datas e pega top 5 distintas\n",
        "        last5 = (\n",
        "            pd.Series(sorted(audit[\"DATA\"].dropna().unique()))\n",
        "            .sort_values()\n",
        "            .tail(5)\n",
        "            .tolist()\n",
        "        )\n",
        "        audit_5d = audit[audit[\"DATA\"].isin(last5)].copy()\n",
        "    else:\n",
        "        # se n√£o houver datas, usa tudo como uma janela fict√≠cia\n",
        "        audit_5d = audit.copy()\n",
        "        audit_5d[\"DATA\"] = pd.Timestamp(\"today\").date()\n",
        "\n",
        "    # ==============================\n",
        "    # TABELAS DA AUDITORIA ‚Äì 5 DIAS\n",
        "    # ==============================\n",
        "\n",
        "    st.subheader(\"üîé Auditoria ‚Äì 5 dias (acertos, perdas e testes)\")\n",
        "\n",
        "    # 1) Tabela por carga (janela 5d)\n",
        "    cols_order = [\n",
        "        \"DATA\",\"IDVEICULO\",\n",
        "        \"CUSTO_FROTA_PRORIA\",\"CUSTO_FROTA_AGREGADA\",\"CUSTO_CROSS\",\n",
        "        \"C_ESC\",\"C_MIN\",\n",
        "        \"NOMETRANSPORTADORACRIACAO\",\"MODAL_MIN\",\"SUCCESS\",\"DELTA_R$\",\"DELTA_%\"\n",
        "    ]\n",
        "    cols_present = [c for c in cols_order if c in audit_5d.columns]\n",
        "    st.markdown(\"**Tabela por carga (janela 5 dias)**\")\n",
        "    st.dataframe(audit_5d[cols_present].sort_values([\"DATA\",\"IDVEICULO\"]))\n",
        "\n",
        "    # 2) Agregados di√°rios\n",
        "    def _agg_day(g: pd.DataFrame):\n",
        "        n = len(g)\n",
        "        acc = g[\"SUCCESS\"].mean() * 100.0 if n else np.nan\n",
        "        perda_media = g[\"DELTA_R$\"].mean() if n else np.nan\n",
        "        perda_total = g[\"DELTA_R$\"].sum() if n else np.nan\n",
        "        mediana = g[\"DELTA_R$\"].median() if n else np.nan\n",
        "        q25 = g[\"DELTA_R$\"].quantile(0.25) if n else np.nan\n",
        "        q75 = g[\"DELTA_R$\"].quantile(0.75) if n else np.nan\n",
        "        return pd.Series({\n",
        "            \"n_cargas\": n,\n",
        "            \"Acc_dia_%\": acc,\n",
        "            \"Perda_M√©dia_Carga_R$\": perda_media,\n",
        "            \"Perda_Total_R$\": perda_total,\n",
        "            \"Mediana_Delta_R$\": mediana,\n",
        "            \"p25_Delta_R$\": q25,\n",
        "            \"p75_Delta_R$\": q75\n",
        "        })\n",
        "\n",
        "    by_day = audit_5d.groupby(\"DATA\", dropna=False).apply(_agg_day).reset_index()\n",
        "    st.markdown(\"**Agregados di√°rios (5 dias)**\")\n",
        "    st.dataframe(by_day.sort_values(\"DATA\"))\n",
        "\n",
        "    # 3) Sum√°rio 5 dias\n",
        "    total_cargas = len(audit_5d)\n",
        "    acc_micro = audit_5d[\"SUCCESS\"].mean() * 100.0 if total_cargas else np.nan\n",
        "    acc_macro = by_day[\"Acc_dia_%\"].mean() if len(by_day) else np.nan\n",
        "    acc_macro_sd = by_day[\"Acc_dia_%\"].std(ddof=1) if len(by_day) > 1 else 0.0\n",
        "    perda_total_5d = audit_5d[\"DELTA_R$\"].sum()\n",
        "    perda_media_carga = audit_5d[\"DELTA_R$\"].mean() if total_cargas else np.nan\n",
        "    perda_media_pct = audit_5d[\"DELTA_%\"].mean() if total_cargas else np.nan\n",
        "\n",
        "    kpi = pd.DataFrame({\n",
        "        \"Acc_micro_%\":[acc_micro],\n",
        "        \"Acc_macro_%\":[acc_macro],\n",
        "        \"DesvPad_Acc_dia\":[acc_macro_sd],\n",
        "        \"Perda_total_5d_R$\":[perda_total_5d],\n",
        "        \"Perda_m√©dia_por_carga_R$\":[perda_media_carga],\n",
        "        \"Perda_m√©dia_%\":[perda_media_pct]\n",
        "    })\n",
        "    st.markdown(\"**KPIs ‚Äì janela 5 dias**\")\n",
        "    st.dataframe(kpi)\n",
        "\n",
        "    # ==================\n",
        "    # TESTES ESTAT√çSTICOS\n",
        "    # ==================\n",
        "    st.markdown(\"### üß™ Testes estat√≠sticos\")\n",
        "\n",
        "    # -- t pareado (DELTA_R$ vs 0) --\n",
        "    delta_vals = audit_5d[\"DELTA_R$\"].dropna().values\n",
        "    t_res, p_res, ci_low, ci_high, cohend = None, None, None, None, None\n",
        "    if len(delta_vals) >= 2 and stats is not None:\n",
        "        m = float(np.mean(delta_vals))\n",
        "        s = float(np.std(delta_vals, ddof=1))\n",
        "        n = len(delta_vals)\n",
        "        t_stat = m / (s / sqrt(n)) if s > 0 else np.inf\n",
        "        df = n - 1\n",
        "        # unicaudal (H1: m√©dia > 0)\n",
        "        p_val = 1 - stats.t.cdf(t_stat, df)\n",
        "        # IC 95% bilateral\n",
        "        t_crit = stats.t.ppf(0.975, df)\n",
        "        ci_low = m - t_crit * s / sqrt(n)\n",
        "        ci_high = m + t_crit * s / sqrt(n)\n",
        "        cohend = m / s if s > 0 else np.inf\n",
        "        t_res, p_res = t_stat, p_val\n",
        "\n",
        "        st.write(f\"**t pareado (ŒîR$ > 0):** t = {t_stat:.3f}, df = {df}, p = {p_val:.4f}\")\n",
        "        st.write(f\"**IC 95% para m√©dia(ŒîR$):** [{ci_low:.2f}, {ci_high:.2f}] | **Cohen's d** = {cohend:.3f}\")\n",
        "    elif len(delta_vals) < 2:\n",
        "        st.info(\"Amostra insuficiente para t-teste (precisa de pelo menos 2 cargas com ŒîR$).\")\n",
        "    else:\n",
        "        st.info(\"Biblioteca estat√≠stica ausente; tente novamente (o app tentou instalar scipy).\")\n",
        "\n",
        "    # -- teste binomial de acerto (micro): H0: p = p0 (ex.: 0.5) --\n",
        "    p0 = 0.5\n",
        "    k_success = int(audit_5d[\"SUCCESS\"].sum())\n",
        "    n_tot = int(total_cargas)\n",
        "    if n_tot >= 1 and stats is not None:\n",
        "        # aproxima√ß√£o normal para binomial (quando n grande) ou usar statsmodels/scipy se desejado\n",
        "        try:\n",
        "            from statsmodels.stats.proportion import proportion_confint\n",
        "            ci_lo, ci_hi = proportion_confint(k_success, n_tot, alpha=0.05, method=\"beta\")  # Clopper‚ÄìPearson\n",
        "        except Exception:\n",
        "            # Wilson manual (fallback)\n",
        "            z = 1.959963984540054\n",
        "            phat = k_success / n_tot if n_tot else 0.0\n",
        "            denom = 1 + z**2 / n_tot\n",
        "            center = phat + z*z/(2*n_tot)\n",
        "            half = z*sqrt((phat*(1-phat)+z*z/(4*n_tot))/n_tot)\n",
        "            ci_lo = (center - half)/denom\n",
        "            ci_hi = (center + half)/denom\n",
        "\n",
        "    # ==================\n",
        "    # GR√ÅFICOS (Altair)\n",
        "    # ==================\n",
        "    try:\n",
        "        # 1) Linha: acur√°cia por dia\n",
        "        chart_acc = alt.Chart(by_day.assign(DATA=by_day[\"DATA\"].astype(str))).mark_line(point=True).encode(\n",
        "            x=alt.X('DATA:N', title='Data'),\n",
        "            y=alt.Y('Acc_dia_%:Q', title='Acur√°cia do dia (%)')\n",
        "        ).properties(title='Acur√°cia di√°ria (5 dias)')\n",
        "        st.altair_chart(chart_acc, use_container_width=True)\n",
        "\n",
        "        # 2) Barras: perda total por dia\n",
        "        chart_perda = alt.Chart(by_day.assign(DATA=by_day[\"DATA\"].astype(str))).mark_bar().encode(\n",
        "            x=alt.X('DATA:N', title='Data'),\n",
        "            y=alt.Y('Perda_Total_R$:Q', title='Perda total (R$)')\n",
        "        ).properties(title='Perda total por dia (R$)')\n",
        "        st.altair_chart(chart_perda, use_container_width=True)\n",
        "\n",
        "        # 3) Boxplot: ŒîR$ por dia\n",
        "        chart_box = alt.Chart(audit_5d.assign(DATA=audit_5d[\"DATA\"].astype(str))).mark_boxplot().encode(\n",
        "            x=alt.X('DATA:N', title='Data'),\n",
        "            y=alt.Y('DELTA_R$:Q', title='ŒîC = C_ESC - C_MIN (R$)')\n",
        "        ).properties(title='Distribui√ß√£o de ŒîR$ por dia')\n",
        "        st.altair_chart(chart_box, use_container_width=True)\n",
        "    except Exception as e:\n",
        "        st.warning(f\"N√£o foi poss√≠vel renderizar os gr√°ficos Altair: {e}\")\n",
        "\n",
        "\n",
        "    # Export all dataframes to different sheets in one Excel file\n",
        "    output = io.BytesIO()\n",
        "    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
        "        out.to_excel(writer, sheet_name='Comparativo Custos', index=False)\n",
        "        df_detalhe.to_excel(writer, sheet_name='Detalhe Frota Propria', index=False)\n",
        "        df_tempo.to_excel(writer, sheet_name='Detalhe Tempo Jornada', index=False)\n",
        "        df_economia.to_excel(writer, sheet_name='Analise Economia Operacional', index=False)\n",
        "\n",
        "        # >>> NOVAS ABAS ‚Äì AUDITORIA 5 DIAS <<<\n",
        "        try:\n",
        "            audit_5d[cols_present].to_excel(writer, sheet_name='Auditoria_5d_Cargas', index=False)\n",
        "        except Exception:\n",
        "            audit_5d.to_excel(writer, sheet_name='Auditoria_5d_Cargas', index=False)\n",
        "        by_day.to_excel(writer, sheet_name='Auditoria_5d_Diario', index=False)\n",
        "        kpi.to_excel(writer, sheet_name='KPIs_5d', index=False)\n",
        "\n",
        "\n",
        "    output.seek(0)\n",
        "\n",
        "    st.download_button(\n",
        "        label=\"Baixar Excel Completo\",\n",
        "        data=output,\n",
        "        file_name=\"Comparativo_Custos_Transporte_Completo.xlsx\",\n",
        "        mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
        "    )\n",
        "\n",
        "else:\n",
        "    st.info(\"Carregue planilhas e clique em Executar\")\n",
        "'''\n",
        "\n",
        "launcher_code = r'''\n",
        "import os, subprocess, time, sys\n",
        "from pyngrok import ngrok\n",
        "from PIL import Image # Import Pillow for image handling\n",
        "\n",
        "def ensure_installed():\n",
        "    import pkgutil\n",
        "    pkgs=[\"streamlit\",\"pyngrok\",\"pandas\",\"numpy\",\"requests\",\"xlsxwriter\",\"Pillow\"] # Added Pillow\n",
        "    need=[p for p in pkgs if pkgutil.find_loader(p) is None]\n",
        "    if need: subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",*need])\n",
        "\n",
        "def run():\n",
        "    port=8501\n",
        "    token=os.environ.get(\"NGROK_AUTH_TOKEN\",\"\")\n",
        "    if token: ngrok.set_auth_token(token)\n",
        "\n",
        "    # Disconnect any existing ngrok tunnels\n",
        "    try:\n",
        "        tunnels = ngrok.get_tunnels()\n",
        "        if tunnels:\n",
        "            print(\"Disconnecting existing tunnels...\")\n",
        "            for tunnel in tunnels:\n",
        "                print(f\"  Disconnecting {tunnel.public_url}\")\n",
        "                ngrok.disconnect(tunnel.public_url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error disconnecting tunnels: {e}\")\n",
        "\n",
        "    url=ngrok.connect(port).public_url\n",
        "    print(\"üåê Public URL:\",url)\n",
        "    p=subprocess.Popen([\"streamlit\",\"run\",\"app.py\"])\n",
        "    try:\n",
        "        while True: time.sleep(5)\n",
        "    except KeyboardInterrupt: p.terminate()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    ensure_installed(); run()\n",
        "'''\n",
        "\n",
        "with open(\"app.py\",\"w\",encoding=\"utf-8\") as f: f.write(app_code)\n",
        "with open(\"run_in_colab.py\",\"w\",encoding=\"utf-8\") as f: f.write(launcher_code)\n",
        "\n",
        "print(\"‚úÖ Arquivos criados: app.py e run_in_colab.py\")\n",
        "\n",
        "# === 2Ô∏è‚É£ Instala depend√™ncias e executa Streamlit com ngrok ===\n",
        "!pip install streamlit pyngrok pandas numpy requests xlsxwriter Pillow -q # Added Pillow\n",
        "\n",
        "import os\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"33vthvkGJREkZID7YROACMe1BMT_KWnW3rpre6rUhGJQV282\"  # <- Substitua aqui pelo seu token ngrok\n",
        "\n",
        "!python run_in_colab.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfx2U82DxD/rrov+LKo0Up",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}